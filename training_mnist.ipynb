{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0gzRakXbJSIdRmlMHEDrT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaRAF/MNIST/blob/main/training_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST\n",
        "\n",
        "O objetivo é reproduzir o projeto apresentado no curso da DIO.\n",
        "\n",
        "O projeto tem como objetivo identificar números através de imagens.\n",
        "\n",
        "Link relacionado: http://yann.lecun.com/exdb/mnist/"
      ],
      "metadata": {
        "id": "yP0uVg8crfOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importação das biblotecas\n",
        "import numpy as np\n",
        "import torch #Framework - aliado do Tensorflow, outro framework\n",
        "import torch.nn.functional as nf #Funções para a rede\n",
        "import torchvision #Para visão computacional\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time #Trabalhar com tempo no algortitmo (tempo execução, de rede etc)\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ],
      "metadata": {
        "id": "xaBf5oCjrsQ6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iremos baixar o dataset do MNIST e tranformar o dataset para o tipo de arquivo tensor.\n",
        "\n",
        "O dataset é baixado do Yann Lecun, responsável pela parte de inteligência artifical e metaverso do Facebook.\n",
        "\n",
        "A imagem em tensor possui uma representação relacionada numérica que a representa de uma forma mais apropriada para trabalhar com redes de deep learning que rodam no TensorFlow estão mais habituadas a trabalhar e, por isso, se torna mais eficiente. \n",
        "\n",
        "Ler mais sobre tensor: https://cursos.alura.com.br/forum/topico-transformar-imagem-em-um-tensor-179620\n"
      ],
      "metadata": {
        "id": "EY5x4WHlubx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #Definindo a conversão de imagens para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) #Carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) #Carrega a parte de validação do dataset\n",
        "trainloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) #Cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "Vc1wqN78szLu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificar se a imagem está sendo corretamente chamada, no caso verificaremos apenas uma imagem."
      ],
      "metadata": {
        "id": "xJr0HT1HzzqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader) #Função inter() ler uma imagem do dataset\n",
        "imagens, etiquetas = dataiter.next() #Chamar o conjunto de imagens\n",
        "\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "heod9dTUx240",
        "outputId": "79e75054-f481-4414-edcd-afe769202cf7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f68ffaa0dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificar as dimensões do tensor da imagem e da etiqueta."
      ],
      "metadata": {
        "id": "KgmpTtCy1S_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #Relação da imagem com o tensor que a representa\n",
        "print(etiquetas[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YyVtm9K1Sf3",
        "outputId": "57efa382-eb20-44f9-de43-2fc2fcdc44be"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos começar a rede!\n",
        "\n",
        "O modelo usado será o modelo Inception V3, também usado pelo Facebook para o reconhecimento facial.\n",
        "\n",
        "Para usar o modelo não precisamos programar ele do zero, pois o encontramos pronto no site https://keras.io/api/applications/.\n",
        "\n",
        "Há vários modelos já existentes, alguns desses modelos podem ser encontrados no site informado acima."
      ],
      "metadata": {
        "id": "AdDHXm-y1y-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Montando a estrutura da rede\n",
        "\n",
        "class Modelo(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) #Camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64)#Camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) #Camada interna 2, 64 neurônios que se ligam a 10\n",
        "    #Para a camada de saída não é necessário definir nada, pois só precisamos pegar o output da camada interna 2\n",
        "  \n",
        "  def foward(self, X):\n",
        "    X = nf.relu(self.linear1(X)) #Função de ativação da camada de entrada para a camada interna 1\n",
        "    X = nf.relu(self.linear2(X)) #Função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) #Função de ativação de camada interna 2 para a camada de saída, nesse caso f(x) = X - ativa as camadas internas da rede\n",
        "    return nf.log_softmax(X, dim=1) #Dados utilizados para calcular a perda "
      ],
      "metadata": {
        "id": "2D--NFWk3fmh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Otimização da rede:\n",
        "\n",
        "Atualização de peso da rede - para isso, precisamos de um otimizador.\n",
        "\n",
        "O otimizador é quem efetivamente treina a rede.\n",
        "\n",
        "Observação: o ideal para o treinamento, é no mínimo 100 épocas. No exemplo usamos apenas 10 para ser mais rápido o desenvolvimento do mesmo."
      ],
      "metadata": {
        "id": "slbEDW7x7x5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Estrutura de treinamento\n",
        "\n",
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) #Define a política de atualização dos pesos e da bias\n",
        "  inicio = time() #Timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss() #Definindo o critério para calcular a perda\n",
        "  EPOCHS = 10 #Número de epochs que o algoritmo rodará\n",
        "  modelo.train() #Ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 #Inicializando a perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "      \n",
        "      imagens = imagens.view(imagens.shape[0], -1) #Convertendo as imagens em \"vetores\" de 28*28 casas para ficarem compatíveis com a \n",
        "      otimizador.zero_grad() #Zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) #Colocando os dados do modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) #Calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() #Back propagation a partir da perda\n",
        "\n",
        "      otimizador().step() #Atualizando os pesos e bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() #Atualização da perda acumulada\n",
        "    \n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "  print(\"\\nTempo de treino (em minutos) = \", (time()-inicio)/60)"
      ],
      "metadata": {
        "id": "8gWBFjCg8o3G"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, criar o modelo de validação."
      ],
      "metadata": {
        "id": "TVY6YP39BIQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784) # 784, pois 28*28\n",
        "      #Desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos têm um custo alto de processamento\n",
        "      with torch.nograd():\n",
        "        logps = modelo(img.to(device)) #Output do modelo em escala logaritmica\n",
        "    \n",
        "    ps = torch.exp(logps) #Converte output para escala normal (lembrando que é um tensor)\n",
        "    probab = list(ps.cpu().numpy()[0])\n",
        "    etiqueta_pred = probab.index(max(probab)) #Converte o tensor em um número, no caso, o número que o modelo previu ---\n",
        "    etiqueta_certa = etiquetas.numpy()[i]\n",
        "\n",
        "    if (etiqueta_certa == etiqueta_pred):\n",
        "      conta_corretas += 1\n",
        "    conta_todas += 1\n",
        "  print(\"Total de imagens testadas = \", conta_todas)\n",
        "  print(f\"\\nPrecisão do modelo = {conta_corretas*100/conta_todas}%\")"
      ],
      "metadata": {
        "id": "WVh3DYzFBHsq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos fazer a leitura do modelo que rodará na rede."
      ],
      "metadata": {
        "id": "rlLJhaylFOpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo() #Chamando o Modelo para executar - inicializa do modelo de treinamento\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Modelo rodará na GPU se possível - verifica se é possível roda no cuda, se tiver uma nvideo dará, se não será no CPU\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_GrXsF_FUeX",
        "outputId": "64cb659f-b2da-422a-b406-769f2f85b42c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}